[config]
# No need to change.
# Autosubmit version identifier
AUTOSUBMIT_VERSION =
# Maximum number of jobs to be waiting in the HPC queue
# Default = 3
MAXWAITINGJOBS = 3 
# Maximum number of jobs to be running at the same time at the HPC
# Default = 6
TOTALJOBS = 6 
# Initial value of submitted jobs
# Default = 0
ALREADYSUBMITTED = 0
# Time between connections to the HPC queue scheduler to poll already submitted jobs status
# Default = 10
SAFETYSLEEPTIME = 10
# Number of retrials if a job fails
# Default = 4
RETRIALS = 4
# Experiment identifier
EXPID =
# Verbose mode to see system output messages while running
VERBOSE = true
# Debug mode to see debug messages while running
# Default = false
DEBUG = false
# Where is Autosubmit default path to store files
ROOTDIR = /cfu/autosubmit/%(EXPID)s

# LOGGING(DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOGLEVEL = DEBUG

# Name of the logging file:
# Default = would be great to be able to pass the PID of the job through the
# name of the logfile --> THAT WILL BE POSSIBLE AT RUNTIME
LOGNAME = %(EXPID)s.log

# On the HPC = where are the jobs touching files, writing logs.
# Default ~/%(EXPID)s_LOG_DIR
HPCFILEDIR = ~/%(EXPID)s_LOG_DIR

# Is Autosubmit running remotely or locally:(with ssh or not)?
# Default = remote
RUNMODE = remote

# IF RUNMODE=Remote = default connection/transfer command:ssh, scp,rsync
# if key files,alias have to be set its here.

# Where is Autosubmit putting the touch files, scripts, temp files ect.
# Default  %(ROOTDIR)s/tmp/
AUTOFILEDIR = %(ROOTDIR)s/tmp/

# Where is Autosubmit reading the experiment definition file
# Default  %(ROOTDIR)s/conf/expdef_%(EXPID)s.conf
EXPDEFFILE = %(ROOTDIR)s/conf/expdef_%(EXPID)s.conf
# Where is Autosubmit reading the architecture definition file.
# Default  %(ROOTDIR)s/conf/archdef_%(EXPID)s.conf
ARCHDEFFILE = %(ROOTDIR)s/conf/archdef_%(EXPID)s.conf

# Autosubmit HEADER for remote jobs
AS-HEADER-REM=
# autosubmit header
 set -xuve
 job_name_ptrn=%SCRATCH_DIR%/%HPCPROJ%/%HPCUSER%/%EXPID%/LOG_%EXPID%/%JOBNAME%
 job_cmd_stamp=$(stat -c %Z $job_name_ptrn.cmd)
 job_start_time=$(date +%s)
 job_queue_time=$((job_start_time - job_cmd_stamp))

# Autosubmit TAILER for remote jobs
AS-TAILER-REM=
# autosubmit tailer
 job_end_time=$(date +%s)
 job_run_time=$((job_end_time - job_start_time))
 if [[ ! -a ${job_name_ptrn}_COMPLETED ]]; then
  times_job_failed=$(($(($(($(ls -1 ${job_name_ptrn}* | wc -l) - 1)) / 2)) - 1))
 else
  times_job_failed=$(($(($(($(ls -1 ${job_name_ptrn}* | wc -l) - 2)) / 2)) - 1))
 fi
 case $HPCARCH in
  hector) times_job_failed=$((times_job_failed + 1)) ;;
 esac
 echo "$job_queue_time $job_run_time $times_job_failed" > ${job_name_ptrn}_COMPLETED

# Autosubmit HEADER for local jobs
AS-HEADER-LOC=
# autosubmit header
 set -xuve
 job_name_ptrn=/cfu/autosubmit/%EXPID%/tmp/LOG_%EXPID%/%JOBNAME%
 job_cmd_stamp=$(stat -c %Z $job_name_ptrn.cmd)
 job_start_time=$(date +%s)
 job_queue_time=$((job_start_time - job_cmd_stamp))

# Autosubmit TAILER for local jobs
AS-TAILER-LOC=
# autosubmit tailer
 job_end_time=$(date +%s)
 job_run_time=$((job_end_time - job_start_time))
 if [[ ! -a ${job_name_ptrn}_COMPLETED ]]; then
  times_job_failed=$(($(($(($(ls -1 ${job_name_ptrn}* | wc -l) - 1)) / 2)) - 1))
 else
  times_job_failed=$(($(($(($(ls -1 ${job_name_ptrn}* | wc -l) - 2)) / 2)) - 1))
 fi
 echo "0 $job_run_time $times_job_failed" > ${job_name_ptrn}_COMPLETED
