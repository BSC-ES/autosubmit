How to checkout
===============

The AUTOSUBMiT code is maintained in an internal CFU GIT repository. The repository is divided in 2 main branches: master and develop. The master branch contains the current stable version. The develop branch contains pre-release unstable version. Branches starting by the word develop- contains different development versions. In tags the old stable versions are kept. To download the code to be able to start working you first need to have an account in the CFU network. If you don't have one please ask CFU IT.

The following command, which has an optional parameter to specify the destination directory, will download all AUTOSUBMiT GIT directories.

::

	git clone https://dev.cfu.local/autosubmit.git <localdir>

To switch to  one branch different from master, it has to be specified. For example:
``git checkout develop``

Introduction
============

After downloading the master branch you will see that there are five directories: ``conf``, ``src``, ``headers``, ``templates`` and ``postp``.  The ``conf`` directory contains two files: ``autosubmit.conf`` and ``expdef.conf``

	* ``autosubmit.conf`` contains the variables needed to run AUTOSUBMiT such as the total number of jobs running at the same time, maximum waiting jobs, time between loop checks, etc.
	* ``expdef.conf`` has the variables needed to configure the experiment such as number of chunks, chunk size, member, etc and the variables needed to configure the climate model.

The ``conf`` directory has also a subfolder archdef with a file for each supported platform

	* ``<platform>.conf``  specify the remote directories (models, scratch, hsm, ...) used to run the experiment on the specified platform.

The ``src`` directory contains all the python files used to create an experiment, launch it, recover and monitor. The ``headers`` folder has a set of files named ``<platform>.<type>`` needed for the queue manager, for each job type for the different platforms. 
Finally, the ``templates`` folder has a set of subfolders for the different models (ecearth, ecearth3, nemo, ifs)  The different subfolders contain the body files, i.e. the shell script to run, for each job type (setup, init, sim, post, clean and trans) that are platform independent.

How to use it
=============

::

	cd <autosubmit_folder>/src

* First Step:

This command creates the directory structure needed to run an experiment, an assigns de experiment id.

::

	python expid.py -h

	python expid.py --new ecearth --HPC ithaca --description "experiment is about..."

For example, "cxxx" is 4 character based expid generated by system.  The first character "c" will represent the platform such as "i" for ithaca, “m” for Marenostrum III, “e” for ECMWF, “l” for Lindgren, "b" for old BSC, "h" for HECToR and so on and rest of three characters alfa-numeric characters identify uniquely the experiment.
At this stage a new folder with the given expid has been created in
``/cfu/autosubmit`` containing all files needed to create a new experiment.

* Second Step:

Define your experiment and configure autosubmit.

::

	vi /cfu/autosubmit/cxxx/conf/expdef_cxxx.conf
	vi /cfu/autosubmit/cxxx/conf/autosubmit_cxxx.conf

* Third Step:

Using the ``expdef_<expid>.conf`` it will generate the experiment. The experiment, which contains all the jobs and its dependencies, will be saved as a pkl file. More info on pickle can be found at http://docs.python.org/library/pickle.html.

::

	python create_exp.py cxxx

In the process of creating the new experiment a plot of the experiment has been created.
It can be found in ``/cfu/autosubmit/<expid>/plot/``

* Fourth Step:

After filling the experiment configuration and running “create_exp.py”, user will find ``/cfu/autosubmit/<expid>/sources``.

Now the user can modify the sources under home infrastructure. If the variable SETUP in ``expdef_<expid>.conf`` is set to TRUE for the experiment (cxxx) then a first setup job will take care of transferring the modified sources at HPC, re-compiling the model and preparing new set of executables. On the other hand, a second setup job will prepare the executables which already exist at HPC.

* Fifth Step:

Launch AUTOSUBMiT in background and with ``nohup`` (continue running although the user who launched the process logs out).

::

	nohup python autosubmit.py cxxx >& cxxx_01.log &

How to monitor the experiment
=============================

The following procedure could be adopted to generate the plots for visualizing the status of the experiment at any instance:

:: 

	cd <autosubmit_folder>/src

* First step:

With this command we can generate new plots to check which is the status of the experiment. Different job status are represented with different colors.::

	python monitor.py -h

	python monitor.py -e cxxx -j job_list -o pdf

or

::

	python monitor.py -e cxxx -j job_list -o png

The location where user can find the generated plots with date and timestamp can be found below:

::

	/cfu/autosubmit/<expid>/plot/<expid>_<date>_<time>.pdf
	
or

::

	/cfu/autosubmit/<expid>/plot/<expid>_<date>_<time>.png

How to change the job status without stopping autosubmit
========================================================

Create a file in ``/cfu/autosubmit/<expid>/pkl/`` named ``updated_list_<expid>.txt``.
This file should have two columns: the first one has to be the job_name and the second one the status (READY, COMPLETED, FAILED). Keep in mind that autosubmit
reads the file automatically so it is suggested to create the file in another location like ``/tmp`` or ``/var/tmp`` and then copy/move it to the ``pkl`` folder. Alternativelly you can create the file with a different name an rename it when you have finished.

How to stop autosubmit
======================

There are currently two ways of stopping AUTOSUBMiT by sending signals to the processes.
To get the process identifier (PID) you can use the ps command on a shell interpreter/terminal.
To send a signal to a process you can use kill also on a terminal.

More info on signals:
http://en.wikipedia.org/wiki/Signal_(computing)

The two signals have their normal behaviour overwritten and new routines have been coded:

* SIGINT: When notified, AUTOSUBMiT will cancel all submitted (queing, running) jobs and stop.
* SIGQUIT: The routine implemented by this signal does a smart stop. This means that it will wait, to stop itself, until all current submitted jobs are finished. It is highly recommended to resynchronize COMPLETED files before relaunching the experiment.

::

	ps -ef |grep [a]utosubmit
	vguemas  22835     1  1 Sep09 ?        00:45:35 python autosubmit.py b02h
	vguemas  25783     1  1 Sep09 ?        00:42:25 python autosubmit.py b02i

To stop immediately experiment b02h:

::

	kill –SIGINT 22835

How to restart
==============

This procedure allows you to modify the pickle without having any knowledge of python.  
You must execute 

::
	
	python recovery.py -h
	
to read help. This script has two mandatory argument that is -e with which we can specify the experiment id and -j with which we can specify the pickle containing the list of jobs.
The -g flag is used to synchronize our experiment locally with the information available on the remote platform (i.e.: download the COMPLETED files we may not have). In case new files are found, the pkl will be updated although we do not specify the -s options, as the information provided is reliable. In addition, every time we run this script, it will check if ``updated_list_<expid>.txt`` exists on the ``pkl`` directory. In case that file exist, it will generate a new plot, without saving the results in the pkl, with the changes specified in the file. When we are satisfied with the results we can use the parameter -s, which will save the change to the pkl file and rename the update file.

How to rerun/extend experiment
==============================

This procedure allows you to create automatically a new pickle with a list of jobs to rerun or an extension of the experiment.
Using the ``expdef_<expid>.conf`` the "create_exp.py" command will generate the rerun if the variable RERUN is set to TRUE and a CHUNKLIST is provided. 

::

	python create_exp.py cxxx

It will read the list of chunks specified in the CHUNKLIST and will generate a new plot, saving the results in the new pkl ``rerun_job_list.pkl``.

Then we are able to start again Autosubmit:

::

	nohup python autosubmit.py cxxx >& cxxx_02.log &

Monitor for rerun:
------------------

::

	python monitor.py -e cxxx -j rerun_job_list -o pdf

Recovery for rerun:
-------------------

::

	python recovery.py -e cxxx -j rerun_job_list -g 

	python recovery.py -e cxxx -j rerun_job_list -s

