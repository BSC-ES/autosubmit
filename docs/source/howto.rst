Introduction
============

Autosubmit is a tool to create, manage and monitor experiments by using Computing Clusters, HPC's and Supercomputers
remotely via ssh. It has support for experiments running in more than one HPC and for different workflow configurations.


For help about how to use autosubmit and a list of available commands execute

::

    autosubmit -h

    usage: autosubmit [-h] [-v]
                  [-lf {EVERYTHING,DEBUG,INFO,RESULT,USER_WARNING,WARNING,ERROR,CRITICAL,NO_LOG}]
                  [-lc {EVERYTHING,DEBUG,INFO,RESULT,USER_WARNING,WARNING,ERROR,CRITICAL,NO_LOG}]

                  {run,expid,delete,monitor,stats,clean,recovery,check,create,configure,install,change_pkl,test,refresh}
                  ...

    Main executable for autosubmit.

    positional arguments:
      {run,expid,delete,monitor,stats,clean,recovery,check,create,configure,install,change_pkl,test,refresh}

    optional arguments:
      -h, --help            show this help message and exit
      -v, --version         returns autosubmit's version number and exit
      -lf {EVERYTHING,DEBUG,INFO,RESULT,USER_WARNING,WARNING,ERROR,CRITICAL,NO_LOG}, --logfile {EVERYTHING,DEBUG,INFO,RESULT,USER_WARNING,WARNING,ERROR,CRITICAL,NO_LOG}
                            sets file's log level.
      -lc {EVERYTHING,DEBUG,INFO,RESULT,USER_WARNING,WARNING,ERROR,CRITICAL,NO_LOG}, --logconsole {EVERYTHING,DEBUG,INFO,RESULT,USER_WARNING,WARNING,ERROR,CRITICAL,NO_LOG}
                            sets console's log level

Execute autosubmit <command> -h for detailed help for each command

How to install
===============

The AUTOSUBMIT code is maintained in PyPi, the main source for python packages.

To install autosubmit just execute

::

	pip install autosubmit

After that, execute
::

    autosubmit configure -h

    usage: autosubmit configure [-h] [-db DATABASEPATH] [-lr LOCALROOTPATH]
                                [-qc QUEUESCONFPATH] [-jc JOBSCONFPATH] [-u | -l]

    configure database and path for autosubmit. It can be done at machine, user or
    local level (by default at machine level)

    optional arguments:
      -h, --help            show this help message and exit
      -db DATABASEPATH, --databasepath DATABASEPATH
                            path to database. If not supplied, it will prompt for
                            it
      -lr LOCALROOTPATH, --localrootpath LOCALROOTPATH
                            path to store experiments. If not supplied, it will
                            prompt for it
      -qc QUEUESCONFPATH, --queuesconfpath QUEUESCONFPATH
                            path to queues.conf file to use by default. If not
                            supplied, it will not prompt for it
      -jc JOBSCONFPATH, --jobsconfpath JOBSCONFPATH
                            path to jobs.conf file to use by default. If not
                            supplied, it will not prompt for it
      -u, --user            configure only for this user
      -l, --local           configure only for using Autosubmit from this path

    autosubmit configure -u

and introduce path to experiment storage and database. Folders must exit.

If not database is created on the given path, execute
::

    autosubmit install -h
    usage: autosubmit install [-h]

    install database for autosubmit on the configured folder

    optional arguments:
      -h, --help  show this help message and exit

    autosubmit install

Now you are ready to use autosubmit

How to use it
=============
* First Step:

This command creates the directory structure needed to run an experiment, an assigns de experiment id.

::

    autosubmit expid -h
    
    usage: autosubmit expid [-h] [-y COPY | -dm] -H HPC -d DESCRIPTION

    Creates a new experiment
    
    optional arguments:
      -h, --help            show this help message and exit
      -y COPY, --copy COPY  makes a copy of the specified experiment
      -dm, --dummy          creates a new experiment with default values, usually
                            for testing
      -H HPC, --HPC HPC     specifies the HPC to use for the experiment
      -d DESCRIPTION, --description DESCRIPTION
                            sets a description for the experiment to store in the
                            database.

    examples: 
    	autosubmit expid --HPC ithaca --description "experiment is about..."
        autosubmit expid --copy i001 --HPC ithaca -d "experiment is about..."
        

For example, "cxxx" is 4 character based expid generated by system.  The first character "c" will represent the platform such as "i" for Ithaca, “m” for Marenostrum III, “e” for ECMWF, “l” for Lindgren, "h" for HECToR and so on and rest of three characters alfa-numeric characters identify uniquely the experiment.
At this stage a new folder with the given expid has been created in
``<experiments_directory>`` containing all files needed to create a new experiment.

At this stage, an automatic check of compatibility between AUTOSUBMIT and given set of templates will be performed. The table of compatibility is a txt file in the local root directory ``<experiments_directory>``. If the check fails, a compatible set of templates should be cloned before going to next steps. The check can be run again by using:

::
    
    python check_compatibility.py -h
    
    usage: check_compatibility.py -e cxxx
    

* Second Step:

Define your experiment and configure AUTOSUBMIT.

::

	vi <experiments_directory>/cxxx/conf/expdef_cxxx.conf
	vi <experiments_directory>/cxxx/conf/autosubmit_cxxx.conf

* Third Step:

Using the ``expdef_<expid>.conf`` it will generate the experiment. The experiment, which contains all the jobs and its dependencies, will be saved as a pkl file. More info on pickle can be found at http://docs.python.org/library/pickle.html.

::

    autosubmit create -h

    usage: autosubmit create [-h] [-np] expid

    create specified experiment joblist

    positional arguments:
      expid          experiment identifier

    optional arguments:
      -h, --help     show this help message and exit
      -np, --noplot  omit plot


    autosubmit create cxxx

In the process of creating the new experiment a plot of the experiment has been created.
It can be found in ``<experiments_directory>/<expid>/plot/``

* Fourth Step:

After filling the experiment configuration and running “create_exp.py”, user can go into ``<experiments_directory>/<expid>/git`` which has a git clone for the model, the ocean diagnostics and the model. Templates has a set of subfolders for the different models (ecearth -version 2-, ecearth3, nemo, ifs -version 2-, ifs3) and one common subfolder. The different subfolders contain the body files, i.e. the shell script to run, for each job type (setup, init, sim, post, clean and trans) that are platform independent.

Additionally the user can modify the sources under git folder. A first setup job will take care of transferring the modified sources at HPC, re-compiling the model and preparing new set of executables. On the other hand, a second setup job will prepare the executables which already exist at HPC. 

The executable scripts are created at runtime (Fifth step) so the modifications on the sources can be done on the fly.

* Fifth Step:

Launch AUTOSUBMIT in background and with ``nohup`` (continue running although the user who launched the process logs out).

::

    autosubmit run -h

    usage: autosubmit run [-h] expid

    runs specified experiment

    positional arguments:
      expid       experiment identifier

    optional arguments:
      -h, --help  show this help message and exit


    nohup autosubmit run cxxx >& cxxx_01.log &

* Cautions: 

Before launching AUTOSUBMIT check the following stuff:

:: 

    ssh localhost # password-les ssh is feasible
    ssh HPC # say for example similarly check other HPC's where password-less ssh is feasible

After launching AUTOSUBMIT, one must be aware of login expeiry limit and policy (if applicable for any HPC) 
and renew the login access accordingly (by using token/key etc) before expiry.


How to monitor the experiment
=============================

The following procedure could be adopted to generate the plots for visualizing the status of the experiment at any instance:

:: 

	cd <autosubmit_folder>/src

* First step:

With this command we can generate new plots to check which is the status of the experiment. Different job status are represented with different colors.::

	autosubmit monitor -h

    usage: autosubmit monitor [-h] [-o {pdf,png,ps}] expid

    plots specified experiment

    positional arguments:
      expid                 experiment identifier

    optional arguments:
      -h, --help            show this help message and exit
      -o {pdf,png,ps}, --output {pdf,png,ps}
                            chooses type of output for generated plot

	autosubmit monitor cxxx 

or

::

	autosubmit monitor  cxxx  -o png

The location where user can find the generated plots with date and timestamp can be found below:

::

	<experiments_directory>/<expid>/plot/<expid>_<date>_<time>.pdf
	
or

::

	<experiments_directory>/<expid>/plot/<expid>_<date>_<time>.png


The following command could be adopted to generate the plots for visualizing the simulation jobs statistics of the experiment at any instance:

:: 

	autosubmit statistics -h

The location where user can find the generated plots with date and timestamp can be found below:

::

	<experiments_directory>/<expid>/plot/<expid>_statistics_<date>_<time>.pdf


How to change the job status without stopping autosubmit
========================================================

Create a file in ``<experiments_directory>/<expid>/pkl/`` named ``updated_list_<expid>.txt``.
This file should have two columns: the first one has to be the job_name and the second one the status (READY, COMPLETED, FAILED, SUSPENDED). Keep in mind that autosubmit
reads the file automatically so it is suggested to create the file in another location like ``/tmp`` or ``/var/tmp`` and then copy/move it to the ``pkl`` folder. Alternativelly you can create the file with a different name an rename it when you have finished.


How to change the job status stopping autosubmit
================================================

This procedure allows you to modify the pickle without having any knowledge of python. Beware that Autosubmit must be stopped to use ``change_pkl.py``. 
You must execute 

::

    autosubmit change_pkl -h

    usage: autosubmit change_pkl [-h] [-s] -t
                             {READY,COMPLETED,WAITING,SUSPENDED,FAILED,UNKNOWN}
                             (-l LIST | -f)
                             [-fc FILTER_CHUNKS | -fs {Any,READY,COMPLETED,WAITING,SUSPENDED,FAILED,UNKNOWN} | -ft FILTER_TYPE]
                             expid

    change job status for an experiment

    positional arguments:
      expid                 experiment identifier

    optional arguments:
      -h, --help            show this help message and exit
      -s, --save            Save changes to disk
      -t {READY,COMPLETED,WAITING,SUSPENDED,FAILED,UNKNOWN}, --status_final {READY,COMPLETED,WAITING,SUSPENDED,FAILED,UNKNOWN}
                            Supply the target status
      -l LIST, --list LIST  Alternative 1: Supply the list of job names to be
                            changed. Default = "Any". LIST =
                            "b037_20101101_fc3_21_sim b037_20111101_fc4_26_sim"
      -f, --filter          Alternative 2: Supply a filter for the job list. See
                            help of filter arguments: chunk filter, status filter
                            or type filter
      -fc FILTER_CHUNKS, --filter_chunks FILTER_CHUNKS
                            Supply the list of chunks to change the status.
                            Default = "Any". LIST = "[ 19601101 [ fc0 [1 2 3 4]
                            fc1 [1] ] 19651101 [ fc0 [16-30] ] ]"
      -fs {Any,READY,COMPLETED,WAITING,SUSPENDED,FAILED,UNKNOWN}, --filter_status {Any,READY,COMPLETED,WAITING,SUSPENDED,FAILED,UNKNOWN}
                            Select the original status to filter the list of jobs
      -ft FILTER_TYPE, --filter_type FILTER_TYPE
                            Select the job type to filter the list of jobs

	
to read help.

This script has three mandatory arguments.

The first with which we must specify the experiment id,
the -t with which we must specify the target status of the jobs we want to change to ``{READY,COMPLETED,WAITING,
SUSPENDED,FAILED,UNKNOWN}``.

The third argument has two alternatives, the -l and -f with which we can apply a filter for the jobs we want to change.

The -l flag recieves a list of jobnames separated by blank spaces (i.e. ``"b037_20101101_fc3_21_sim b037_20111101_fc4_26_sim"``) same as in the previous ``updated_list_<expid>.txt``.
If we supply the key word "Any", all jobs will be changed to the target status.

The -f flag can be used in three modes: the chunk filter, the status filter or the type filter.

* The variable -fc should be a list of individual chunks or ranges of chunks in the following format: ``"[ 19601101 [ fc0 [1 2 3 4] fc1 [1] ] 19651101 [ fc0 [16-30] ] ]"``

* The variable -fs can be one of the following status for job: ``{Any,READY,COMPLETED,WAITING,SUSPENDED,FAILED,UNKNOWN}``

* The variable -ft can be one of the defined types of job.

When we are satisfied with the results we can use the parameter -s, which will save the change to the pkl file.

How to stop autosubmit
======================

There are currently two ways of stopping AUTOSUBMIT by sending signals to the processes.
To get the process identifier (PID) you can use the ps command on a shell interpreter/terminal.
To send a signal to a process you can use kill also on a terminal.

More info on signals:
http://en.wikipedia.org/wiki/Signal_(computing)

The two signals have their normal behaviour overwritten and new routines have been coded:

* SIGINT: When notified, AUTOSUBMIT will cancel all submitted (queing, running) jobs and stop.
* SIGQUIT: The routine implemented by this signal does a smart stop. This means that it will wait, to stop itself, until all current submitted jobs are finished. It is highly recommended to resynchronize COMPLETED files before relaunching the experiment.

::

	ps -ef |grep [a]utosubmit
	vguemas  22835     1  1 Sep09 ?        00:45:35 autosubmit run b02h
	vguemas  25783     1  1 Sep09 ?        00:42:25 autosubmit run b02i

To stop immediately experiment b02h:

::

	kill –SIGINT 22835

How to restart
==============

This procedure allows you to modify the pickle without having any knowledge of python.  
You must execute 

::
	
	python recovery.py -h
	
to read help. This script has two mandatory argument that is -e with which we can specify the experiment id and -j with which we can specify the pickle containing the list of jobs.

The -g flag is used to synchronize our experiment locally with the information available on the remote platform (i.e.: download the COMPLETED files we may not have). In case new files are found, the pkl will be updated although we do not specify the -s options, as the information provided is reliable.

In addition, every time we run this script, it will check if ``updated_list_<expid>.txt`` exists on the ``pkl`` directory. In case that file exist, it will generate a new plot, without saving the results in the pkl, with the changes specified in the file. 

When we are satisfied with the results we can use the parameter -s, which will save the change to the pkl file and rename the update file.

How to rerun/extend experiment
==============================

This procedure allows you to create automatically a new pickle with a list of jobs to rerun or an extension of the experiment.
Using the ``expdef_<expid>.conf`` the "create_exp.py" command will generate the rerun if the variable RERUN is set to TRUE and a CHUNKLIST is provided. 

::

	autosubmit create cxxx

It will read the list of chunks specified in the CHUNKLIST and will generate a new plot, saving the results in the new pkl ``rerun_job_list.pkl``.

Then we are able to start again Autosubmit:

::

    nohup autosubmit run cxxx >& cxxx_02.log &


How to clean an experiment
==========================


This procedure allows you to save space after finalising an experiment.  
You must execute 

::

    autosubmit clean -h
	

to read help. 

This script has one mandatory argument with which we can specify the experiment id.

* The -p flag is used to clean our experiment ``plot`` folder to save disk space. Only the two latest plots will be kept. Older plots will be removed.
* The -g flag is used to clean our experiment ``git`` clone locally in order to save space (``model`` is particullary big). 

A bare copy (which occupies less space on disk) will be automatically made. That bare clone can be always reconverted in a working clone if we want to run again the experiment by using ``git clone bare_clone original_clone``.

Bear in mind that if we have not synchronized our experiment git folder with the information available on the remote repository (i.e.: commit and push any changes we may have), or in case new files are found, the clean procedure will be failing although we specify the -g option.

In addition, every time we run this script with -g option, it will check the commit SHA for local working tree of
``model``, ``template`` and ``ocean_diagnostics`` existing on the ``git`` directory. In case that commit SHA exist, finalise_exp will register it to the database along with the branch name.