How to checkout
===============

The AUTOSUBMiT code is maintained in an internal CFU GIT repository. The repository is divided in 2 main branches: master and develop. The master branch contains the current stable version. The develop branch contains pre-release unstable version. Branches starting by the word develop- contains different development versions. In tags the old stable versions are kept. To download the code to be able to start working you first need to have an account in the CFU network. If you don't have one please ask CFU IT.

The following command, which has an optional parameter to specify the destination directory, will download all AUTOSUBMiT GIT directories.

::

	git clone https://dev.cfu.local/autosubmit.git <localdir>

To switch to one previous tag from current stable version, it has to be specified. For example:
``git checkout autosubmit2.1``
To switch to one branch different from master, it has to be specified. For example:
``git checkout develop``


Since version AUTOSUBMiT 2.2 several new internal CFU GIT projects contain the climate model sources and setup, the templates and the ocean_diagnostics.
Since version AUTOSUBMiT 2.3 only templates version 1.1 or above can be used.
Only in case you want to develop any modification of the templates, the source code or the setup you will download them with the following commands:

::

	git clone https://dev.cfu.local/<model_name>.git <localdir>
	git clone https://dev.cfu.local/templates.git <localdir>
	git clone https://dev.cfu.local/ocean_diagnostics.git <localdir>

All available GIT projects and branches can be found at https://dev.cfu.local portal.
More information on how to work with CFU GIT projects can be found here: :doc:`dev`


Introduction
============

After downloading the master branch you will see that there are three directories: ``conf``, ``src`` and ``docs``.  The ``conf`` directory contains two files: ``autosubmit.conf`` and ``expdef.conf``

	* ``autosubmit.conf`` contains the variables needed to run AUTOSUBMiT such as the total number of jobs running at the same time, maximum waiting jobs, time between loop checks, etc.
	* ``expdef.conf`` has the variables needed to configure the experiment such as number of chunks, chunk size, member, etc and the variables needed to configure the climate model.

The ``conf`` directory has also a subfolder archdef with a file for each supported platform

	* ``<platform>.conf``  specify the remote directories (models, scratch, hsm, ...), environtment settings and headers used to run the experiment on the specified platform.

The ``src`` directory contains all the python files used to create an experiment, launch it, recover and monitor. 

Finally, the ``docs`` folder has the documentation files a README and this user's guide in PDF version.



How to use it
=============
::

	cd <autosubmit_folder>/src

* First Step:

This command creates the directory structure needed to run an experiment, an assigns de experiment id.

::

	python expid.py -h
	
	usage: expid.py [-h] (--new | --copy COPY)
			--HPC {bsc,hector,ithaca,lindgren,ecmwf,marenostrum3}
			--model_name {ecearth,nemo}
			--model_branch MODEL_BRANCH {master (default), v3.0.1, v2.3.0, develop-v3.0.1, develop-v2.3.0, ...}
			--template_name {ecearth,ecearth3,ifs,ifs3,nemo} 
			[--template_branch TEMPLATE_BRANCH] {master (defualt), develop, develop-SPPT, ...}
			[--ocean_diagnostics_branch OCEAN_DIAGNOSTICS_BRANCH] {master (defualt), develop, ...}
			--description DESCRIPTION

	examples: 
		python expid.py --new --HPC ithaca --model_name ecearth --model_branch v2.3.0 --template_name ecearth --description "experiment is about..."
		python expid.py --new --HPC ithaca --model_name ecearth3 --model_branch develop-v3.0.1-nudging --template_name ecearth3 
						--template_branch develop-nudging --description "experiment is about..."

For example, "cxxx" is 4 character based expid generated by system.  The first character "c" will represent the platform such as "i" for Ithaca, “m” for Marenostrum III, “e” for ECMWF, “l” for Lindgren, "h" for HECToR and so on and rest of three characters alfa-numeric characters identify uniquely the experiment.
At this stage a new folder with the given expid has been created in
``/cfu/autosubmit`` containing all files needed to create a new experiment.

At this stage, an automatic check of comptaibility between AUTOSUBMiT and given set of templates will be performed. The table of comptaibility is a txt file in the local root directory ``/cfu/autosubmit``. If the check fails, a compatible set of templates should be cloned before going to next steps. The check can be run again by using:

::
    
    python check_compatibility.py -h
    
    usage: check_compatibility.py -e cxxx
    

* Second Step:

Define your experiment and configure AUTOSUBMiT.

::

	vi /cfu/autosubmit/cxxx/conf/expdef_cxxx.conf
	vi /cfu/autosubmit/cxxx/conf/autosubmit_cxxx.conf

* Third Step:

Using the ``expdef_<expid>.conf`` it will generate the experiment. The experiment, which contains all the jobs and its dependencies, will be saved as a pkl file. More info on pickle can be found at http://docs.python.org/library/pickle.html.

::

	python create_exp.py cxxx

In the process of creating the new experiment a plot of the experiment has been created.
It can be found in ``/cfu/autosubmit/<expid>/plot/``

* Fourth Step:

After filling the experiment configuration and running “create_exp.py”, user can go into ``/cfu/autosubmit/<expid>/git`` which has a git clone for the model, the ocean diagnostics and the model. Templates has a set of subfolders for the different models (ecearth -version 2-, ecearth3, nemo, ifs -version 2-, ifs3) and one common subfolder. The different subfolders contain the body files, i.e. the shell script to run, for each job type (setup, init, sim, post, clean and trans) that are platform independent.

Additionally the user can modify the sources under git folder. A first setup job will take care of transferring the modified sources at HPC, re-compiling the model and preparing new set of executables. On the other hand, a second setup job will prepare the executables which already exist at HPC. 

The executable scripts are created at runtime (Fifth step) so the modifications on the sources can be done on the fly.

* Fifth Step:

Launch AUTOSUBMiT in background and with ``nohup`` (continue running although the user who launched the process logs out).

::

	nohup python autosubmit.py cxxx >& cxxx_01.log &

How to monitor the experiment
=============================

The following procedure could be adopted to generate the plots for visualizing the status of the experiment at any instance:

:: 

	cd <autosubmit_folder>/src

* First step:

With this command we can generate new plots to check which is the status of the experiment. Different job status are represented with different colors.::

	python monitor.py -h

	python monitor.py -e cxxx -j job_list -o pdf

or

::

	python monitor.py -e cxxx -j job_list -o png

The location where user can find the generated plots with date and timestamp can be found below:

::

	/cfu/autosubmit/<expid>/plot/<expid>_<date>_<time>.pdf
	
or

::

	/cfu/autosubmit/<expid>/plot/<expid>_<date>_<time>.png

How to change the job status without stopping autosubmit
========================================================

Create a file in ``/cfu/autosubmit/<expid>/pkl/`` named ``updated_list_<expid>.txt``.
This file should have two columns: the first one has to be the job_name and the second one the status (READY, COMPLETED, FAILED, SUSPENDED). Keep in mind that autosubmit
reads the file automatically so it is suggested to create the file in another location like ``/tmp`` or ``/var/tmp`` and then copy/move it to the ``pkl`` folder. Alternativelly you can create the file with a different name an rename it when you have finished.


How to change the job status stopping autosubmit
================================================

This procedure allows you to modify the pickle without having any knowledge of python. Beware that Autosubmit must be stopped to use ``change_pkl.py``. 
You must execute 

::
	
	python change_pkl.py -h
	
to read help. This script has four mandatory arguments.
The -e with which we can specify the experiment id, 
the -j with which we can specify the pickle containing the list of jobs, 
the -t with which we can specify the target status of the jobs we want to change to {READY,COMPLETED,WAITING,SUSPENDED,FAILED,UNKNOWN}.
The fourth argument has two alternatives, the -l and -f with which we can apply a filter for the jobs we want to change.
The -l flag recieves a list of jobnames separated by blank spaces (i.e. "b037_20101101_fc3_21_sim b037_20111101_fc4_26_sim") same as in the previous ``updated_list_<expid>.txt``.
If we supply the key word "Any", all jobs will be changed to the target status.
The -f flag can be used in three modes: the chunk filter, the status filter or the type filter.
The variable -fc should be a list of individual chunks or ranges of chunks in the following format: "[ 19601101 [ fc0 [1 2 3 4] fc1 [1] ] 19651101 [ fc0 [16-30] ] ]"
The variable -fs can be one of the following status for job: {Any,READY,COMPLETED,WAITING,SUSPENDED,FAILED,UNKNOWN}
The variable -ft can be one of the following types of job: {Any,LOCALSETUP,REMOTESETUP,INITIALISATION,SIMULATION,POSTPROCESSING,CLEANING,LOCALTRANSFER} 
When we are satisfied with the results we can use the parameter -s, which will save the change to the pkl file.

How to stop autosubmit
======================

There are currently two ways of stopping AUTOSUBMiT by sending signals to the processes.
To get the process identifier (PID) you can use the ps command on a shell interpreter/terminal.
To send a signal to a process you can use kill also on a terminal.

More info on signals:
http://en.wikipedia.org/wiki/Signal_(computing)

The two signals have their normal behaviour overwritten and new routines have been coded:

* SIGINT: When notified, AUTOSUBMiT will cancel all submitted (queing, running) jobs and stop.
* SIGQUIT: The routine implemented by this signal does a smart stop. This means that it will wait, to stop itself, until all current submitted jobs are finished. It is highly recommended to resynchronize COMPLETED files before relaunching the experiment.

::

	ps -ef |grep [a]utosubmit
	vguemas  22835     1  1 Sep09 ?        00:45:35 python autosubmit.py b02h
	vguemas  25783     1  1 Sep09 ?        00:42:25 python autosubmit.py b02i

To stop immediately experiment b02h:

::

	kill –SIGINT 22835

How to restart
==============

This procedure allows you to modify the pickle without having any knowledge of python.  
You must execute 

::
	
	python recovery.py -h
	
to read help. This script has two mandatory argument that is -e with which we can specify the experiment id and -j with which we can specify the pickle containing the list of jobs.
The -g flag is used to synchronize our experiment locally with the information available on the remote platform (i.e.: download the COMPLETED files we may not have). In case new files are found, the pkl will be updated although we do not specify the -s options, as the information provided is reliable. In addition, every time we run this script, it will check if ``updated_list_<expid>.txt`` exists on the ``pkl`` directory. In case that file exist, it will generate a new plot, without saving the results in the pkl, with the changes specified in the file. When we are satisfied with the results we can use the parameter -s, which will save the change to the pkl file and rename the update file.

How to rerun/extend experiment
==============================

This procedure allows you to create automatically a new pickle with a list of jobs to rerun or an extension of the experiment.
Using the ``expdef_<expid>.conf`` the "create_exp.py" command will generate the rerun if the variable RERUN is set to TRUE and a CHUNKLIST is provided. 

::

	python create_exp.py cxxx

It will read the list of chunks specified in the CHUNKLIST and will generate a new plot, saving the results in the new pkl ``rerun_job_list.pkl``.

Then we are able to start again Autosubmit:

::

	nohup python autosubmit.py cxxx >& cxxx_02.log &

Monitor for rerun:
------------------

::

	python monitor.py -e cxxx -j rerun_job_list -o pdf

Recovery for rerun:
-------------------

::

	python recovery.py -e cxxx -j rerun_job_list -g 

	python recovery.py -e cxxx -j rerun_job_list -s

